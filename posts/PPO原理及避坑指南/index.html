<!DOCTYPE html><html lang="zh" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="<p>这里只有冷静思考，没有浮夸喧哗 </p> 这里只有算法与代码，没有装饰和繁杂">
      
      
        <meta name="author" content="Xiaochuan Zhang">
      
      
        <link rel="canonical" href="https://matrixmind.fun/posts/PPO%E5%8E%9F%E7%90%86%E5%8F%8A%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/">
      
      
      
      
      <link rel="icon" href="../../assets/images/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.21">
    
    
      
        <title>PPO原理及避坑指南 - MatrixMind.Fun</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2a3383ac.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Noto Sans SC";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/brand.css">
    
      <link rel="stylesheet" href="https://unpkg.com/glightbox/dist/css/glightbox.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="cyan">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-ppo" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="MatrixMind.Fun" class="md-header__button md-logo" aria-label="MatrixMind.Fun" data-md-component="logo">
      
  <img src="../../assets/images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MatrixMind.Fun
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PPO原理及避坑指南
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="cyan" aria-label="深色模式" type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="深色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="cyan" aria-label="浅色模式" type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="浅色模式" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m3.55 19.09 1.41 1.41 1.8-1.79-1.42-1.42M12 6c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6c0-3.32-2.69-6-6-6m8 7h3v-2h-3m-2.76 7.71 1.8 1.79 1.41-1.41-1.79-1.8M20.45 5l-1.41-1.4-1.8 1.79 1.42 1.42M13 1h-2v3h2M6.76 5.39 4.96 3.6 3.55 5l1.79 1.81zM1 13h3v-2H1m12 9h-2v3h2"></path></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/zhangxiaochuan" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link">
        
  
  
    
  
  Posts

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../tags/" class="md-tabs__link">
        
  
  
    
  
  Tags

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../projects/" class="md-tabs__link">
        
  
  
    
  
  Projects

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="MatrixMind.Fun" class="md-nav__button md-logo" aria-label="MatrixMind.Fun" data-md-component="logo">
      
  <img src="../../assets/images/logo.png" alt="logo">

    </a>
    MatrixMind.Fun
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/zhangxiaochuan" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Posts
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tags/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tags
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projects
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      1. PPO 算法原理
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-ppo-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      2. PPO 的 PyTorch 实现
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 常见陷阱与调试建议
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/zhangxiaochuan/edit/main/docs/posts/PPO原理及避坑指南.md" title="编辑此页" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg>
    </a>
  
  


  <h1>PPO原理及避坑指南</h1>

<div class="mmf-tags">
  <span class="mmf-tags-title">标签：</span>
  <a class="mmf-tag" href="/tags/强化学习/" title="强化学习">强化学习</a><a class="mmf-tag" href="/tags/llm/" title="LLM">LLM</a><a class="mmf-tag" href="/tags/post-training/" title="Post Training">Post Training</a>
</div>

<h2 id="1-ppo">1. PPO 算法原理<a class="headerlink" href="#1-ppo" title="Permanent link">¶</a></h2>
<p><strong>背景与动机：</strong> Proximal Policy Optimization（PPO）算法由 OpenAI 的 John Schulman 等人在 2017 年提出。它属于策略梯度方法的一种改进，旨在解决标准策略梯度方法不稳定、大步更新容易导致性能崩溃的问题。在此之前，Trust Region Policy Optimization（TRPO）算法通过在策略更新时加入严格的 KL 散度约束来保证策略每次只做小幅度改进，从而实现稳定的单调提升，但 TRPO 需要二阶导数和复杂的共轭梯度算法实现，计算代价高且实现复杂。PPO 的提出正是为了<strong>在保持策略更新稳定性的同时，简化实现难度</strong>。实验表明，PPO 在采样效率、性能稳定性上可媲美 TRPO，但<strong>只需使用一阶优化方法（如 SGD 或 Adam）即可实现</strong>，无需复杂的二阶优化。</p>
<p><strong>PPO 两种策略更新方式：</strong> PPO 实际上是一个策略优化方法的家族，主要包括两种变体：一是带<strong>KL惩罚项</strong>的方式（PPO-Penalty），即在目标函数中加入 KL 散度惩罚因子，动态调整其权重，使新旧策略的 KL 散度维持在期望范围；二是更常用的<strong>截断策略目标</strong>（PPO-Clip），即直接在目标函数中对新旧策略的概率比率进行裁剪，以限制每次更新的幅度。实践中，PPO-Clip 因为实现简单、稳定性好而成为主流，也就是 OpenAI 等机构使用的默认版本。下面将重点介绍截断版 PPO 的核心思想和目标函数设计。</p>
<p><strong>截断目标函数：</strong> PPO-Clip 的关键在于定义<strong>概率比率</strong><span class="arithmatex">\(r_t(\theta)\)</span>来度量新旧策略在同一动作上的概率变化幅度：</p>
<div class="arithmatex">\[r_t(\theta) \;=\; \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} ,\]</div>
<p>其中<span class="arithmatex">\(\pi_{\theta}\)</span>是当前策略，<span class="arithmatex">\(\pi_{\theta_{\text{old}}}\)</span>是策略更新前的旧策略。如果<span class="arithmatex">\(r_t(\theta) &gt; 1\)</span>，表示新策略在状态<span class="arithmatex">\(s_t\)</span>下让动作<span class="arithmatex">\(a_t\)</span>更加趋向于发生；反之若<span class="arithmatex">\(r_t(\theta) &lt; 1\)</span>，则新策略降低了该动作的概率。为了避免<span class="arithmatex">\(r_t(\theta)\)</span>偏离1过多（对应策略改变太大），PPO 引入了<strong>截断剪切操作</strong>。截断版策略梯度的目标函数可以表示为：</p>
<div class="arithmatex">\[L_{\text{CLIP}}(\theta) \;=\; \mathbb{E}_t\Big[\,\min\big(r_t(\theta)\,\hat{A}_t,\;\text{clip}\big(r_t(\theta),\,1-\varepsilon,\,1+\varepsilon\big)\,\hat{A}_t\big)\Big],\]</div>
<p>其中<span class="arithmatex">\(\hat{A}_t\)</span>是优势函数的估计，<span class="arithmatex">\(\varepsilon\)</span>是一个小的超参数（例如 0.1 或 0.2）。这个目标函数取原始概率比率优势<span class="arithmatex">\(r_t(\theta)\hat{A}_t\)</span>和<strong>截断后的概率比率优势</strong>两者中的较小值，从而实现对策略更新幅度的限制。当优势<span class="arithmatex">\(\hat{A}_t\)</span>为正时，如果<span class="arithmatex">\(r_t(\theta)\)</span>尝试增大超过<span class="arithmatex">\((1+\varepsilon)\)</span>，截断操作会阻止策略优势继续变大；当优势为负时，若<span class="arithmatex">\(r_t(\theta)\)</span>降低到小于<span class="arithmatex">\((1-\varepsilon)\)</span>，截断也会阻止策略过度减小该动作概率。通过这种方式，PPO <strong>在目标函数层面内嵌了对策略更新幅度的限制</strong>，近似实现了 TRPO 的“信赖域”效果，而无需显式计算二阶梯度或进行约束优化。</p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../assets/images/PPO%E5%8E%9F%E7%90%86%E5%8F%8A%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/1754381285053-07a63cbe-1817-410a-849d-ccb6680cdc5c.png" data-desc-position="bottom"><img alt="" src="../../assets/images/PPO%E5%8E%9F%E7%90%86%E5%8F%8A%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/1754381285053-07a63cbe-1817-410a-849d-ccb6680cdc5c.png"></a></p>
<p>图1：PPO 截断策略目标示意图。左图表示优势<span class="arithmatex">\(A\)</span>为正的情形，右图为优势<span class="arithmatex">\(A\)</span>为负时的情形，红点表示策略更新的起始位置（<span class="arithmatex">\(r=1\)</span> 对应旧策略）。</p>
<p>通过上述截断策略，PPO 的<strong>策略优化更加保守稳定</strong>：当策略更新对提升收益非常有利时，也不会因为贪心而走得太远；当策略更新方向是有害的，截断机制则会给予显著的惩罚，促使梯度反向推动策略朝纠正方向发展。值得一提的是，PPO 论文也探讨了使用<strong>自适应 KL 惩罚</strong>的方法（PPO-Penalty），即给目标函数添加<span class="arithmatex">\(\beta \cdot \mathrm{KL}(\pi_{\theta_{\text{old}}}|\pi_{\theta})\)</span>惩罚项并动态调整系数<span class="arithmatex">\(\beta\)</span>控制新旧策略距离。然而实验证明剪切法（PPO-Clip）在实现简单性的同时，能取得更好的稳定性能，因此 PPO-Clip 更为常用。</p>
<p><strong>与 TRPO 的比较：</strong> 总的来说，PPO 延续了 TRPO“限制策略改变量”的思想，都旨在防止策略更新步长过大引起性能崩溃。不同之处在于 TRPO 通过二阶近似和约束优化严格限制 KL 散度，而 PPO 则通过修改目标函数（剪切或软约束）实现类似效果，<strong>算法实现更加简单高效</strong>。这使得 PPO 更易于在复杂策略网络（例如带共享参数的 actor-critic 架构）中应用，并支持直接用自动微分框架实现梯度更新。实践经验表明，PPO 通常更容易调参，适用范围更广，而收敛性能与 TRPO 相当甚至更优。凭借在连续控制、离散动作（Atari 游戏）等各种任务中的出色表现，PPO 已成为深度强化学习领域<strong>应用最广泛的策略优化算法</strong>之一。</p>
<h2 id="2-ppo-pytorch">2. PPO 的 PyTorch 实现<a class="headerlink" href="#2-ppo-pytorch" title="Permanent link">¶</a></h2>
<p><strong>整体结构：</strong> PPO 通常采用 Actor-Critic 架构实现，其中包含策略网络（Actor）和价值网络（Critic）。可以将 Actor 和 Critic 集成在<strong>同一个模型</strong>中，共享部分层然后输出各自的结果，也可以使用两个独立的神经网络。一个经典设计是共享一个隐层表示，然后 Actor 输出动作分布参数（对于连续动作环境通常输出均值和对数标准差，用于构造高斯分布；对于离散动作则输出各动作的概率），Critic 输出对应状态的价值估计<span class="arithmatex">\(V(s)\)</span>。下面是一段 PPO 算法核心训练步骤的 PyTorch 风格代码示例（为便于说明做了简化，省略了并行采样、熵奖励等工程细节）：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 假设 actor_critic 是包含策略和价值的模型，已获得采样数据：old_states, old_actions, old_log_probs, returns, advantages</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>                          <span class="c1"># 对同一批数据进行多轮更新</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">old_states</span><span class="p">))</span> <span class="c1"># 打乱索引实现小批量</span>
    <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_states</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">mb_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span> <span class="p">:</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
        <span class="n">states_b</span> <span class="o">=</span> <span class="n">old_states</span><span class="p">[</span><span class="n">mb_idx</span><span class="p">]</span>
        <span class="n">actions_b</span> <span class="o">=</span> <span class="n">old_actions</span><span class="p">[</span><span class="n">mb_idx</span><span class="p">]</span>
        <span class="n">old_log_b</span> <span class="o">=</span> <span class="n">old_log_probs</span><span class="p">[</span><span class="n">mb_idx</span><span class="p">]</span>
        <span class="n">returns_b</span> <span class="o">=</span> <span class="n">returns</span><span class="p">[</span><span class="n">mb_idx</span><span class="p">]</span>
        <span class="n">adv_b</span>    <span class="o">=</span> <span class="n">advantages</span><span class="p">[</span><span class="n">mb_idx</span><span class="p">]</span>

        <span class="c1"># 前向计算策略和价值</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">logstd</span><span class="p">,</span> <span class="n">value_b</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="p">(</span><span class="n">states_b</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">logstd</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span>   <span class="c1"># 连续动作策略分布</span>
        <span class="n">new_log_probs</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions_b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># 计算概率比率 r(θ)</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">new_log_probs</span> <span class="o">-</span> <span class="n">old_log_b</span><span class="p">)</span>

        <span class="c1"># 计算截断策略损失 L_CLIP</span>
        <span class="n">surr1</span> <span class="o">=</span> <span class="n">ratio</span> <span class="o">*</span> <span class="n">adv_b</span>
        <span class="n">surr2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">clip_param</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">clip_param</span><span class="p">)</span> <span class="o">*</span> <span class="n">adv_b</span>
        <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>           <span class="c1"># 取负号转化为损失（梯度下降最小化）</span>

        <span class="c1"># 计算价值函数损失</span>
        <span class="n">value_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">value_b</span><span class="p">,</span> <span class="n">returns_b</span><span class="p">)</span>

        <span class="c1"># 总损失（熵奖励项此处省略）</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">policy_loss</span> <span class="o">+</span> <span class="n">vf_coef</span> <span class="o">*</span> <span class="n">value_loss</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>  <span class="c1"># 梯度裁剪</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>
<p>上述代码实现了 PPO 更新的核心步骤：首先利用当前策略网络计算给定状态批次的动作概率 <em>new_log_probs</em> 和状态价值 <em>value_b</em>，然后计算新旧策略的概率比率 <code>ratio</code>。接着构造<strong>未截断</strong>的策略收益 <code>surr1 = ratio * advantage</code> 和<strong>截断后的</strong>收益 <code>surr2 = clamp(ratio, 1-ε, 1+ε) * advantage</code>，取两者的最小值并加负号求平均作为策略损失（即<span class="arithmatex">\(-L_{\text{CLIP}}\)</span>）。价值函数损失则使用MSE将 Critic 输出的价值<span class="arithmatex">\(V(s)\)</span>回归到实际的回报（或称 <em>returns</em>，通常采用TD(lambda)或蒙特卡洛方式计算)。总损失是两者加权和（代码中策略损失和价值损失系数 <em>vf_coef</em> 通常取 1.0；若添加熵奖励还需减去 <em>entropy_coef</em><span class="arithmatex">\(\times\)</span>熵）。然后对总损失执行反向传播，并进行梯度裁剪以增强训练稳定性，最后更新网络参数。</p>
<p><strong>采样与训练流程：</strong> PPO 的训练遵循“交替采样-更新”的循环过程。具体步骤如下：</p>
<ol>
<li><strong>采样数据：</strong> 在当前策略<span class="arithmatex">\(\pi_{\theta_{\text{old}}}\)</span>下，与环境交互采集一批轨迹数据（可以并行多个环境加速收集）。每条轨迹长度为<span class="arithmatex">\(T\)</span>，汇总得到一个包含<span class="arithmatex">\(N\)</span>条数据（状态-动作-奖励序列）的批次。通常 <span class="arithmatex">\(N=\text{batch_size}\)</span>会设得较大以降低策略梯度的方差，如 2048、4096 等。</li>
<li><strong>计算奖励与优势：</strong> 根据收集到的每条轨迹，计算每个时间步的累积折扣回报（称为 <em>返回</em>）。利用价值网络（Critic）估计每个状态的基准值<span class="arithmatex">\(V(s_t)\)</span>，由此计算优势函数估计<span class="arithmatex">\(\hat{A}_t = G_t - V(s_t)\)</span>，其中<span class="arithmatex">\(G_t\)</span>是该状态起始的折扣累计回报。为降低方差、提高优势估计的稳健性，一般使用<strong>广义优势估计</strong>（GAE）方法来平滑计算<span class="arithmatex">\(\hat{A}_t\)</span>。GAE 在计算优势时引入了参数<span class="arithmatex">\(\lambda\)</span>（如 0.95）来折中偏差和方差，使得优势估计更稳定。</li>
<li><strong>构造 PPO 损失：</strong> 将采样得到的数据视为固定数据集，以旧策略<span class="arithmatex">\(\pi_{\theta_{\text{old}}}\)</span>为参考计算概率比<span class="arithmatex">\(r_t(\theta)\)</span>，构建剪切策略目标<span class="arithmatex">\(L_{\text{CLIP}}(\theta)\)</span>和（可选的）价值损失、熵项等，得到完整的 PPO 损失函数（见上文公式和代码实现）。</li>
<li><strong>多轮梯度更新：</strong> 在保持采样数据不变的情况下，使用随机梯度上升（实现上等价于对上述总损失下降）对策略网络和价值网络执行<span class="arithmatex">\(K\)</span>轮更新。每轮从采样批次中按一定 <strong>minibatch</strong> 大小抽取小批数据，计算梯度并更新参数。这样重复多次，以充分利用每批样本。</li>
<li><strong>更新旧策略参数：</strong> 将<span class="arithmatex">\(\theta_{\text{old}} \leftarrow \theta\)</span>，即将新参数作为下次采样的旧策略，继续下一循环。通常重复上述循环直至收敛或训练步数耗尽。</li>
</ol>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../../assets/images/PPO%E5%8E%9F%E7%90%86%E5%8F%8A%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/1754381304739-7cfcb116-4708-4a5c-925e-af264f7cd19a.jpeg" data-desc-position="bottom"><img alt="" src="../../assets/images/PPO%E5%8E%9F%E7%90%86%E5%8F%8A%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/1754381304739-7cfcb116-4708-4a5c-925e-af264f7cd19a.jpeg"></a></p>
<p>图2：PPO 算法整体训练流程示意。Actor 从环境状态<span class="arithmatex">\(s\)</span><em> 产生动作<span class="arithmatex">\(a\)</span></em> 与环境交互收集奖励<span class="arithmatex">\(r\)</span>_，Critic 估计状态价值用于优势计算。经验数据存入回放缓冲（PPO中一般不重复使用旧数据，即 on-policy，每次更新后清空缓冲）。一定步长后，将收集的数据用于多轮策略网络和价值网络的更新（图中“Update”步骤），更新完成后重置旧策略，再次采集数据。</p>
<p>PyTorch 实现中，上述循环常表现为两个嵌套的<code>for</code>循环：外层循环处理采样-更新迭代，内层循环对每批数据进行<span class="arithmatex">\(K\)</span>次小批量优化。需要注意的是，在实现细节上，为了提升性能和效果，通常会<strong>对优势进行标准化</strong>处理（减去均值除以标准差），对每个小批重新计算策略的 log概率和价值以保证对应当前参数，等等。这些工程细节在很多开源实现（如 OpenAI Baselines、CleanRL 等）中有体现，相关研究也总结了 PPO 实现需要注意的诸多细节。</p>
<h2 id="3">3. 常见陷阱与调试建议<a class="headerlink" href="#3" title="Permanent link">¶</a></h2>
<p>尽管 PPO 算法相对稳定和鲁棒，但在实际训练中仍有许多<strong>超参数选择和实现细节</strong>会显著影响效果。以下总结 PPO 实验中常见的陷阱和调优建议：</p>
<ul>
<li><strong>学习率（Learning Rate）：</strong> 学习率过高时，策略更新步伐过大，可能导致训练不稳定甚至收益崩塌；学习率过低则收敛缓慢，难以达到最优表现。一般采用自适应矩估计优化器（Adam）并需根据任务调整学习率，典型取值在<span class="arithmatex">\(10^{-4}\)</span>量级。可以通过监控训练曲线的波动来调节学习率：若观察到策略收益剧烈震荡或下降，应适当降低学习率；反之收敛过慢则可酌情提高。</li>
<li><strong>剪切系数 </strong><span class="arithmatex">\(\varepsilon\)</span><strong>：</strong> 选择合适的裁剪范围对于PPO至关重要。<span class="arithmatex">\(\varepsilon\)</span>通常设置在0.1到0.2之间。如果<span class="arithmatex">\(\varepsilon\)</span>取值过大，策略更新约束变得宽松，PPO 可能退化为未经限制的策略梯度，从而失去稳定性；反之若<span class="arithmatex">\(\varepsilon\)</span>太小，更新步幅将过于保守，导致策略收敛变慢。经验上，<span class="arithmatex">\(\varepsilon=0.2\)</span>常作为默认值，并在此基础上根据 KL 散度等指标微调。</li>
<li><strong>采样批量大小：</strong> 每次迭代收集的样本数量（即 batch size 或者每条轨迹长度<span class="arithmatex">\(T\)</span>）会影响策略梯度估计的方差和稳定性。批量过小会导致估计噪声大，训练曲线抖动明显；批量过大则提高单次迭代计算成本。常用的设置是在一个并行环境步数积累 2048 或 4096 条样本后更新一次。总的原则是：<strong>增大样本量可以换取更平稳的策略提升</strong>，但会降低每单位样本的更新频率，要在计算资源允许范围内平衡二者。</li>
<li><strong>多轮小批更新次数 </strong><span class="arithmatex">\(K\)</span><strong>：</strong> PPO 允许对同一批数据进行多次梯度上升更新，但是更新轮数不能过多。一般设置<span class="arithmatex">\(K=3\sim10\)</span>，如果<span class="arithmatex">\(K\)</span>太大，则可能过度拟合这批采样数据，导致策略对新数据泛化变差。调试时可以观察每次策略更新的<strong>KL散度</strong>或<strong>被截断的样本比例（clip fraction）</strong>：若多轮更新后 KL 散度明显超出预期（例如超过 0.1），或者 clip fraction 很高（大量样本达到了剪切边界），说明策略更新幅度过大，可能需要减少<span class="arithmatex">\(K\)</span>或降低学习率。</li>
<li><strong>值函数估计偏差：</strong> 价值网络（Critic）的准确性对优势函数估计影响重大。如果 Critic 无法精确逼近环境的价值函数，优势估计将出现偏差，进而影响策略梯度方向。常见现象是策略陷入次优或收敛缓慢。应对方法包括：<strong>增加价值网络的训练强度</strong>（例如每次策略更新迭代多次价值网络或者提高价值损失权重<span class="arithmatex">\(c_1\)</span>）以及增加 Critic 网络容量（更多层数或单元）。此外，可以引入<strong>值函数截断</strong>（即限制每次更新价值的变化幅度）来防止价值估计发散，这在某些实现中被证明有助于稳定训练。</li>
<li><strong>熵系数与探索：</strong> PPO 通常会在目标中加入策略熵的奖励项（系数记为<span class="arithmatex">\(c_2\)</span>）以鼓励探索。如果熵系数设置过大，策略可能长期保持高随机性，收益提升缓慢；若熵系数过小，则可能过早收敛到次优策略。实践中<span class="arithmatex">\(c_2\)</span>一般取 0.01～0.02 左右。调参时可以观察策略的<strong>熵值曲线</strong>：熵值下降过快意味着探索不足，可适当提高<span class="arithmatex">\(c_2\)</span>；熵值长期维持较高则考虑降低<span class="arithmatex">\(c_2\)</span>。</li>
<li><strong>训练不稳定及调试：</strong> 当 PPO 训练出现不稳定或收敛困难时，建议从以下方面排查：①<strong>归一化优势</strong>：确保优势经过减均值除方差的标准化处理，可显著稳定训练；②<strong>学习率退火</strong>：随着训练推进逐步降低学习率，避免后期震荡；③<strong>观察关键指标</strong>：定期打印 KL 散度、clip fraction、策略熵和价值损失等指标，判断是否策略更新过度（如KL剧增）或价值函数训练不足等；④<strong>环境噪声和随机种子</strong>：对于结果波动大的任务，多运行几次取平均，以分辨算法本身问题还是随机因素导致。</li>
</ul>
<p>总的来说，PPO 作为当前最成功的策略优化算法之一，其核心思想是在策略梯度更新中引入适度的约束，从而达到<strong>稳定训练与高效采样</strong>的平衡。通过合理的超参数调节和技巧（如GAE、熵奖励、梯度裁剪等），已经在机器人的连续控制、游戏AI以及大规模预训练模型微调（如基于人类反馈的对话模型优化）等众多场景中取得了卓越的成果。</p>







  
  






                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "toc.follow", "navigation.top", "content.code.copy", "content.action.edit", "content.tabs.link", "search.highlight", "search.suggest", "header.autohide"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://unpkg.com/glightbox/dist/js/glightbox.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>